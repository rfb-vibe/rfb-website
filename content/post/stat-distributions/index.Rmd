---
title: 'Statistical Distributions'
author: 'Rebecca Frost-Brewer'
date: '2022-03-28'
slug: []
categories: []
tags: []
output: html_document
---

Code example for finding $\overline{x}$ of five samples:
```{python, eval=FALSE}

five_sample_means = []
for i in range(5):
    sample = df.sample(n = 50, random_state = i + 100)
    five_sample_means.append(sample.Age.mean)

# for a value in the range 1-5, add to the list 'five_sample_means' the average age

```

Code example for a bar plot with a dotted line showing the mean:
```{python, eval=FALSE}

x_labels = [f"Sample {x}" for x in range(1, 6)]

fig, ax = plt.subplots(figsize=(7,6))

ax.bar(x_labels, five_sample_means)
ax.set_ylabel("Mean Age")
ax.axhline(y=population_mean, color="red", linewidth=5, linestyle="--")
ax.legend(
    handles=[Line2D([0],[0], color="red", linestyle="--")],
    labels=["True Population Mean"],
    fontsize="large"
);

```


## The Probability Mass Function

A function that associates probability with discrete random variables.

Measures of central tendency and spread for discrete distributions

* Expected value
* Variance

Example:
The expected student-to-teacher ratio is 32.5 : 1. But randomly interviewed students often feel that their average class size is bigger than 32.5. There are two main reasons for this:

1. Students typically take 4 - 5 classes at any given time, but teachers usually only teach 1 or 2 classes.
2. The number of students in a small class is small, and the number of students in a large class is large.

```{python, eval=FALSE}

import pandas as pd
import numpy as np

# Determine total number of classes (integer value)
sum_class = sum(size_and_count.values())

# Create a pandas Series of all possible outcomes (class sizes)
# Sizes = the list of each class size range
sizes = pd.Series(size_and_count.keys())

# Divide each class size value by the total number of classes
# to create a pandas Series of PMF values
actual_pmf = pd.Series([value/sum_class for value in size_and_count.values()])

# Display probabilities in a dataframe
pmf_df = pd.concat([sizes, actual_pmf], axis=1)
pmf_df.columns = ["Class Size", "Overall Probability"]
pmf_df.style.hide_index()

```


Plot PMF
```{python, eval=FALSE}

import matplotlib.pyplot as plt
%matplotlib inline

plt.style.use('ggplot')
pmf_df.plot.bar(x="Class Size", y="Overall Probability");

```

Write a function to calculate the probability of any input's outcome
```{python, eval=FALSE}

# p_actual - probability of any input's outcome
def p_actual(x_i):

# for any class size value, divided by total number of classes
return size_and_count[x_i] / sum_class

# Return the probability of that outcome
p_actual(17) # 0.13513513513513514

```

Calculate expected value or mean $E(X)$
```{python, eval=FALSE}

# Calculate the expected value (mu) using formula above

# Sizes list is the list of different class sizes

# Multiply each element in the sizes list by their probability of occurrence

# Then sum the resulting values
mu = (sizes.apply(p_actual) * sizes).sum()
mu
# 32.472972972972975

```


## The Probability Density Function

Helps identify the regions in the distribution of a continuous variable where observations are more likely to occur, where the observation occurrence is more dense.

The probability associated with a certain value is expressed as an interval of ranges.

The probability of observing a value within a specific range is calculating the area under the curve of that range.

Code to create a PDF histogram, non-parametric Kernel Density Estimation plot, and parametric distribution fit plot
```{python, eval=FALSE}

import scipy.stats as stats

# Create two vertical subplots sharing 15% and 85% of plot space
# sharex allows sharing of axes i.e. building multiple plots on same axes
fig, (ax, ax2) = plt.subplots(2, sharex=True,
                                gridspec_kw={"height_ratios": (.15, .85)},
                                figsize = (10,8) )

sns.distplot(data.Height,
                hist=True, hist_kws={
                                    "linewidth": 2,
                                    "edgecolor" :'red',
                                    "alpha": 0.4,
                                    "color": "w",
                                    "label": "Histogram",
                                    },
                kde=True, kde_kws = {'linewidth': 3,
                                        'color': "blue",
                                        "alpha": 0.7,
                                        'label':'Kernel Density Estimation                                             Plot'
                                        },
                fit= stats.norm, fit_kws = {'color' : 'green',
                                            'label' : 'parametric fit',
                                            "alpha": 0.7,
                                            'linewidth':3},
                ax=ax2)

ax2.set_title('Density Estimations')

sns.boxplot(x=data.Height, ax = ax,color = 'red')
ax.set_title('Box and Whiskers Plot')
ax2.set(ylim=(0, .08))
plt.ylim(0,0.11)
plt.legend();

```

Plot histograms with overlapping variables
```{python, eval=FALSE}

binsize = 10

male_df.Height.plot.hist(bins = binsize, density = True, alpha = 0.7, label ="Male Height");

female_df.Height.plot.hist(bins = binsize, density = True, alpha = 0.7, label = 'Female Height');

plt.legend()
plt.show()

```

Write a density function
```{python, eval=FALSE}

def density(x):

    n, bins = np.histogram(x, 10, density=1)

    # Initialize numpy arrays with zeros to store interpolated values
    pdfx = np.zeros(n.size)
    pdfy = np.zeros(n.size)

    # Interpolate through histogram bins
    # identify middle point between two neighbouring bins, in terms of x        and y coords
    for k in range(n.size):
        pdfx[k] = 0.5*(bins[k]+bins[k+1])
        pdfy[k] = n[k]

    # plot the calculated curve
    return pdfx, pdfy

# Generate test data and test the function
np.random.seed(5)
mu, sigma = 0, 0.1 # mean and standard deviation
s = np.random.normal(mu, sigma, 100)
x,y = density(s)
plt.plot(x,y, label = 'test')
plt.legend();

```

Overlapping histograms in `seaborn`
```{python, eval=FALSE}

import seaborn as sns

sns.distplot(male_df.Weight)
sns.distplot(female_df.Weight)
plt.title('Comparing Weights')
plt.show()

```


## Cumulative Distribution Function

A function where $x$ is an value that can possibly appear in a given distribution. To calculate the Cumulative Density Function of $x$, compute the proportion of values in the distribution less than or equal to $x$

Write a function to calculate the cumulative density function
```{python, eval=FALSE}

# 'lst' = list of all possible values
# 'X' = value we want to calculate

def calculate_cdf(lst, X):
    count = 0
    # for all values in the list 'lst', if the value is less than or equal         to X,
    # add one to count
    for value in lst:
    if value <= X:
    count += 1

    # calculate cumulative probability of X by dividing the count by the      total number of
    # possible values in the list
    cum_prob = count / len(lst) # normalizing cumulative probabilities (as     with pmfs)
    return round(cum_prob, 3)

# test data
test_lst = [1,2,3]
test_X = 2

calculate_cdf(test_lst, test_X)
# 0.667

```

CDFs are implemented with two sorted lists: one which contains the potential outcome values of the discrete distribution and another with the cumulative probabilities.


## Bernoulli and Binomial Distribution

The Bernoulli experiment is an experiment with a binary outcome: 0-1, success-failure, heads-tails, etc.

The binomial distribution describes the process of performing $n$ independent Bernoulli trials

Example:
When playing a game of bowling, what is the probability of throwing exactly 3 strikes in a game with 10 rounds? Assume that the probability of throwing a strike is 25% for each round.

Write a function for factorial
```{python, eval=FALSE}

def factorial(n):
    prod = 1
    while n >= 1:
        prod = prod * n
        n = n - 1
    return prod

```

```{python, eval=FALSE}

# n = number of trials (10)
# k = number of success/outcomes we want (3)
# p = probability of outcome (.25)

p_3_strikes = (factorial(10)/(factorial(7)*factorial(3)))*(0.25)**3*(0.75)**7

p_3_strikes
#answer = 0.2502822

```

Create a binomial distribution function
```{python, eval=FALSE}

# n = number of trials
# k = number of success/outcomes we want
# p = probability of outcome

def binom_distr(n,p,k):
    p_k = (factorial(n)/(factorial(k)*factorial(n-k)))*(p**k*(1-p)**(n-k))
    return p_k

```


## The Normal Distribution

The bell curve, the Gaussian curve - a continuous distribution that is symmetrical and its mean, median, and mode are equal. The area under the curve is 1.0

68% of values are within 1 standard deviation of the mean
95% of values are within 2 standard deviations of the mean
99.7% of values are within 3 standard deviations of the mean


**The Central Limit Theorem:** When you add a large number of independent random variables, irrespective of the original distribution of these variables, their sum tends towards a normal distribution.

Normal distributions in python
```{python, eval=FALSE}

import numpy as np
import seaborn as sns

mu, sigma = 0.5, 0.1
n = 1000
s = np.random.normal(mu, sigma, n)
sns.distplot(s);

```

Formula to calculate the density function
```{python, eval=FALSE}

# Calculate the normal Density function
density = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2))

```


The Standard Normal Distribution
A special case of the normal distribution with a mean of 0 and standard deviation of 1

Any normal distribution can be converted to a standard normal distribution and vice versa using the equation:

$z$ = $x$ - \mu divided by \sigma

A z-score can help identify how many standard deviations above or below the mean a certain observation is - "above" or "below" the mean


