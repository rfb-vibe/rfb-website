---
title: 'Running Tally of Code and Functions for Data Science'
author: 'Rebecca Frost-Brewer'
date: '2022-03-05'
slug: []
categories: []
tags: []
output:
  html_document:
    code_folding: "hide"
---



<p>This post serves as a reference tool for the code and functions used in Python for data science.</p>
<!--more-->
<p><a href="#bash-and-git">Bash and Git</a><br>
<a href="#base-python">Base Python</a><br>
<a href="#intro-to-pandas">Intro to <code>pandas</code> and <code>matplotlib</code></a><br>
<a href="#sql">SQL</a><br>
<a href="#api-call-example">Conducting an API Call and Analyzing Results</a><br>
<a href="#sets">Sets</a><br>
<a href="#prob-mass-function">The Probability Mass Function</a><br>
<a href="#prob-density-function">The Probability Density Function</a><br>
<a href="#clt">Central Limit Theorem</a><br>
<a href="#effect-size-stat-power">Effect Size and Statistical Power</a>
<a href="#ab-testing">A/B Hypothesis Testing</a></p>
<hr />
<div id="bash-and-git" class="section level2">
<h2>Bash and Git</h2>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>Bash Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>cd</code></td>
<td>Navigate to the directory/folder where you want to work</td>
</tr>
<tr class="even">
<td><code>cd ~/Documents</code></td>
<td>Move to user’s home directory</td>
</tr>
<tr class="odd">
<td><code>pwd</code></td>
<td>Print working directory (see where you’re working in bash)</td>
</tr>
<tr class="even">
<td><code>mkdir</code></td>
<td>Create a new folder within your repo directory</td>
</tr>
<tr class="odd">
<td><code>git init</code></td>
<td>Turn current directory into a Git repository</td>
</tr>
<tr class="even">
<td><code>git clone URL</code></td>
<td>Download a forked repo from your GitHub to your local computer</td>
</tr>
<tr class="odd">
<td><code>git add</code></td>
<td>Stage changes you’ve made so they’re ready to be committed</td>
</tr>
<tr class="even">
<td><code>git status</code></td>
<td>Check out what’s going on in your repo and its files</td>
</tr>
<tr class="odd">
<td><code>git remote add origin</code></td>
<td>Add the link to the GitHub URL where your repository is stored online</td>
</tr>
<tr class="even">
<td><code>git commit -am "your commit message</code></td>
<td>Commit added files to the repository with shortcut <code>am</code> to “add message”</td>
</tr>
<tr class="odd">
<td><code>git push origin master</code></td>
<td>Push/Sync changes to online version of the repo (<code>master</code> is the default branch for all Git repos)</td>
</tr>
<tr class="even">
<td><code>git branch</code></td>
<td>Start a new branch within your repo - good as a “sandbox” version for messing around without fear of changing the <code>master</code></td>
</tr>
<tr class="odd">
<td><code>git checkout</code></td>
<td>Navigate to a different branch</td>
</tr>
<tr class="even">
<td><code>git log</code></td>
<td>Displays the history of commits for the branch you’re on</td>
</tr>
<tr class="odd">
<td><code>git merge &lt;branch-to-merge&gt; -m "message"</code></td>
<td>Merge a branch into the master</td>
</tr>
<tr class="even">
<td><code>ls</code></td>
<td>List all the files in the current directory</td>
</tr>
</tbody>
</table>
<hr />
</div>
<div id="base-python" class="section level2">
<h2>Base Python</h2>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>Python Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>! ls</code></td>
<td>List the files in the current directory</td>
</tr>
<tr class="even">
<td><code>import</code></td>
<td>Load a necessary Python package/module</td>
</tr>
<tr class="odd">
<td><code>! cat</code></td>
<td>Inspect/list the files of a file that was opened</td>
</tr>
<tr class="even">
<td><code>len()</code></td>
<td>Show length (number of) items/records</td>
</tr>
<tr class="odd">
<td><code>print(df)</code></td>
<td>Look a dataframe’s elements</td>
</tr>
<tr class="even">
<td><code>df.head()</code></td>
<td>Show first five rows</td>
</tr>
<tr class="odd">
<td><code>df.tail()</code></td>
<td>Show last five rows</td>
</tr>
<tr class="even">
<td><code>df.info()</code></td>
<td>Show summary of dataframe (columns, values, value type)</td>
</tr>
<tr class="odd">
<td><code>df.index</code></td>
<td>Access index or row labels of dataframe</td>
</tr>
<tr class="even">
<td><code>df.columns</code></td>
<td>Access column labels</td>
</tr>
<tr class="odd">
<td><code>df.dtypes</code></td>
<td>Access data types of all columns</td>
</tr>
<tr class="even">
<td><code>df.shape</code></td>
<td>Returns a tuple representing dimensionality (rows, columns)</td>
</tr>
<tr class="odd">
<td><code>df.iloc[#]</code></td>
<td>Select row based on integer-location</td>
</tr>
<tr class="even">
<td><code>df.loc[:,'column name']</code></td>
<td>Select row based on row index and column name</td>
</tr>
<tr class="odd">
<td><code>df.map()</code></td>
<td>Transforms values</td>
</tr>
<tr class="even">
<td><code>df['column'].value_counts()</code></td>
<td>Get count of records of a particular column</td>
</tr>
<tr class="odd">
<td><code>df.describe()</code></td>
<td>Calculate basic summary statistics of each column</td>
</tr>
<tr class="even">
<td><code>df['column'].mean()</code></td>
<td>Calculate specific summary stat of a specific column</td>
</tr>
<tr class="odd">
<td><code>.mode()</code></td>
<td>the mode of the column</td>
</tr>
<tr class="even">
<td><code>.count()</code></td>
<td>the count of the total number of entries in a column</td>
</tr>
<tr class="odd">
<td><code>.std()</code></td>
<td>the standard deviation for the column</td>
</tr>
<tr class="even">
<td><code>.var()</code></td>
<td>the variance for the column</td>
</tr>
<tr class="odd">
<td><code>.sum()</code></td>
<td>the sum of all values in the column</td>
</tr>
<tr class="even">
<td><code>.cumsum()</code></td>
<td>the cumulative sum, where each cell index contains the sum of all indices lower than, and including, itself.</td>
</tr>
<tr class="odd">
<td><code>df.isna()</code></td>
<td>Return a matrix of boolean values (T/F) where all cells contain NaN</td>
</tr>
<tr class="even">
<td><code>df.groupby()</code></td>
<td>Aggregate function to group by a particular column</td>
</tr>
<tr class="odd">
<td><code>pd.concat()</code></td>
<td>Pandas function to concatenate two or more dataframes</td>
</tr>
<tr class="even">
<td><code>df.join()</code></td>
<td>Join all records from the left table and the right table that have a matching key</td>
</tr>
</tbody>
</table>
<div id="csv" class="section level3">
<h3>CSV</h3>
<pre class="python"><code>import csv

# This code prints the first line of the CSV file
with open(csv_file_path) as csvfile:
    print(csvfile.readline())


# Print OrderedDict from first row of CSV file  - reads each row then converts to dictionary
with open(csv_file_path) as csvfile:
    reader = csv.DictReader(csvfile)
    print(next(reader))</code></pre>
</div>
<div id="json" class="section level3">
<h3>JSON</h3>
<pre class="python"><code>import json

with open(&#39;nyc_2001_campaign_finance.json&#39;) as f:
    data = json.load(f)</code></pre>
<div id="investigating-the-dataset-structure" class="section level4">
<h4>Investigating the dataset structure</h4>
<pre class="python"><code># Data type of dictionary keys`
type(data[&#39;key&#39;])

print(f&quot;The overall data type is {type(data)}&quot;)
print(f&quot;The keys are {list(data.keys())}&quot;)
print(&quot;The value associated with the &#39;meta&#39; key has metadata, including all of these attributes:&quot;)
print(list(data[&#39;meta&#39;][&#39;view&#39;].keys()))
print(f&quot;The value associated with the &#39;data&#39; key is a list of {len(data[&#39;data&#39;])} records&quot;)

# Checking individual data types
for key in data[&#39;json-object&#39;].keys():
    print(key, type(data[&#39;json-object&#39;][key]))

# Each dictionary has a &#39;name key&#39;
# Use list comprehension for column_names
column_names = [column[&#39;name&#39;] for column in data[&#39;meta&#39;][&#39;view&#39;][&#39;columns&#39;]]
print(column_names)</code></pre>
<hr />
</div>
</div>
</div>
<div id="intro-to-pandas" class="section level2">
<h2>Common code examples using <code>pandas</code></h2>
<pre class="python"><code># Import libraries using the standard alias
import pandas as pd

# Load &#39;datafile.csv&#39; as a DataFrame
df = pd.read_csv(&#39;datafile.csv&#39;)

# Group by the column business_id, then number of stars, and take the mean of stars by business id
df.groupby(&#39;business_id&#39;)[&#39;stars&#39;].mean().head()

# Drop duplicates from df
df = df.drop_duplicates()

# Concatenate three dataframes together
combined_df = pd.concat([df1, df2, df3])

# Pivot table
pivoted = grouped.pivot(index = &#39;Pclass&#39;, columns = &#39;Sex&#39;, values = &#39;Age&#39;)

# Select all rows of games played in Group 3 during the 1950 WC
df[(df[&#39;Year&#39;] == 1950) &amp; (df[&#39;Stage&#39;] == &#39;Group 3&#39;)]

# Count number of home games played by the Netherlands
neth_home = len(df[df[&#39;Home Team Name&#39;] == (&#39;Netherlands&#39;)])

# How many countries played in 1986?
wc1986 = df.loc[df[&#39;Year&#39;] == 1986]

# Display all records containing the string &#39;Korea&#39;
df.loc[df[&#39;Home Team Name&#39;].str.contains(&#39;Korea&#39;), &#39;Home Team Name&#39;]

# Type out abbreviations and store as dictionary object
division_mapping = {
    &quot;IRT&quot;: &quot;Interborough Rapid Transit Company&quot;,
    &quot;IND&quot;: &quot;Independent Subway System&quot;,
    &quot;BMT&quot;: &quot;Brooklyn–Manhattan Transit Corporation&quot;,
    &quot;PTH&quot;: &quot;Port Authority Trans-Hudson (PATH)&quot;,
    &quot;SRT&quot;: &quot;Staten Island Rapid Transit&quot;,
    &quot;RIT&quot;: &quot;Roosevelt Island Tram&quot;
}

# For the column DIVISION, map the full names instead of the abbreviations
df[&#39;DIVISION&#39;].map(division_mapping)

# Rename a column, make change permanent
df.rename(columns={&#39;C/A&#39; : &#39;CONTROL_AREA&#39;}, inplace = True)

# Change type of entry of a column
df[&#39;ENTRIES&#39;] = df[&#39;ENTRIES&#39;].astype(int)

# Change date string to date format
pd.to_datetime(df[&#39;DATE&#39;], format=&#39;%m/%d/%Y&#39;).head()

# For the text column, for each item, split each word, and list the length, but show only first five rows
df[&#39;text&#39;].map(lambda review_text: len(review_text.split())).head()

# Sorting by last name
names = [&#39;Miriam Marks&#39;,&#39;Sidney Baird&#39;,&#39;Elaine Barrera&#39;,&#39;Eddie Reeves&#39;,&#39;Marley Beard&#39;,
         &#39;Jaiden Liu&#39;,&#39;Bethany Martin&#39;,&#39;Stephen Rios&#39;,&#39;Audrey Mayer&#39;,&#39;Kameron Davidson&#39;,
         &#39;Teagan Bennett&#39;]

# Split/identify the second word, sort by the second word
sorted(names, key=lambda x: x.split()[1])

# Find the average number of words for a review
df[&#39;text&#39;].map(lambda x: len(x.split())).mean()

# Print the percentage of null values in the column &#39;Cabin
print(&#39;Percentage of Null Cabin Values:&#39;, len(df[df.Cabin.isna()])/ len(df))

# Print the number of unique cabin values
print(&#39;Number of Unique Cabin Values:&#39;, df.Cabin.nunique())

# Replace missing values with a summary statistic
df[&#39;Age&#39;] = df[&#39;Age&#39;].fillna(value=df[&#39;Age&#39;].median)

# Drop rows that contain missing values
df = df.dropna()

# Find summary statistics of grouped columns
df.groupby([&#39;Sex&#39;, &#39;Pclass&#39;]).mean()</code></pre>
<hr />
</div>
<div id="sql" class="section level2">
<h2>SQL</h2>
<table>
<thead>
<tr class="header">
<th>SQL Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>SELECT</code></td>
<td>Select which columns</td>
</tr>
<tr class="even">
<td><code>FROM</code></td>
<td>Select from which table</td>
</tr>
<tr class="odd">
<td><code>pd.read_sql("", conn)</code></td>
<td>Executing a SQL query using pandas</td>
</tr>
<tr class="even">
<td><code>WHERE</code></td>
<td>Rows match a particular condition</td>
</tr>
<tr class="odd">
<td><code>CAST</code></td>
<td>Change record to a particular value type</td>
</tr>
<tr class="even">
<td><code>AS</code></td>
<td>Create an alias to rename a column</td>
</tr>
<tr class="odd">
<td><code>ORDER BY</code></td>
<td>Order query result</td>
</tr>
<tr class="even">
<td><code>ASC</code></td>
<td>Ascending</td>
</tr>
<tr class="odd">
<td><code>DESC</code></td>
<td>Descending</td>
</tr>
<tr class="even">
<td><code>LIMIT</code></td>
<td>Specify a certain number of results</td>
</tr>
<tr class="odd">
<td><code>HAVING</code></td>
<td>Filters conditions after <code>GROUP BY</code></td>
</tr>
</tbody>
</table>
<div id="conditional-sql-operators" class="section level3">
<h3>Conditional SQL Operators</h3>
<ul>
<li><code>!=</code> (“not equal to”)
<ul>
<li>Similar to <code>not</code> combined with <code>==</code> in Python</li>
</ul></li>
<li><code>&gt;</code> (“greater than”)
<ul>
<li>Similar to <code>&gt;</code> in Python</li>
</ul></li>
<li><code>&gt;=</code> (“greater than or equal to”)
<ul>
<li>Similar to <code>&gt;=</code> in Python</li>
</ul></li>
<li><code>&lt;</code> (“less than”)
<ul>
<li>Similar to <code>&lt;</code> in Python</li>
</ul></li>
<li><code>&lt;=</code> (“less than or equal to”)
<ul>
<li>Similar to <code>&lt;=</code> in Python</li>
</ul></li>
<li><code>AND</code>
<ul>
<li>Similar to <code>and</code> in Python</li>
</ul></li>
<li><code>OR</code>
<ul>
<li>Similar to <code>or</code> in Python</li>
</ul></li>
<li><code>BETWEEN</code>
<ul>
<li>Similar to placing a value between two values with <code>&lt;=</code> and <code>and</code> in Python, e.g. <code>(2 &lt;= x) and (x &lt;= 5)</code></li>
</ul></li>
<li><code>IN</code>
<ul>
<li>Similar to <code>in</code> in Python</li>
</ul></li>
<li><code>LIKE</code>
<ul>
<li>Uses wildcards to find similar strings. No direct equivalent in Python, but similar to some Bash terminal commands.</li>
</ul></li>
</ul>
</div>
<div id="sql-examples" class="section level3">
<h3>SQL Examples</h3>
<pre class="python"><code>import sqlite3 
conn = sqlite3.connect(&#39;data.sqlite&#39;) # Create a connection the SQL database

# Select all columns from the employees database
pd.read_sql(&quot;&quot;&quot;
SELECT *
  FROM employees;
&quot;&quot;&quot;, conn)

# Select the lastName and firstName columns from the employee database, show first five rows
pd.read_sql(&quot;&quot;&quot;
SELECT lastName, firstName
  FROM employees;
&quot;&quot;&quot;, conn).head()

# Select firstName, but make an alias (rename the column) &#39;name&#39;, from employees db
pd.read_sql(&quot;&quot;&quot;
SELECT firstName AS name
  FROM employees;
&quot;&quot;&quot;, conn).head()


pd.read_sql(&quot;&quot;&quot;
SELECT firstName, lastName, jobTitle,
       CASE
       WHEN jobTitle = &quot;Sales Rep&quot; THEN &quot;Sales Rep&quot;
       ELSE &quot;Not Sales Rep&quot;
       END AS role
  FROM employees;
&quot;&quot;&quot;, conn).head(10)

# Return the length (number) of character in the string firstName
pd.read_sql(&quot;&quot;&quot;
SELECT length(firstName) AS name_length
  FROM employees;
&quot;&quot;&quot;, conn).head()

# Use CAST to return integers instead of decimals
pd.read_sql(&quot;&quot;&quot;
SELECT CAST(round(priceEach) AS INTEGER) AS rounded_price_int
  FROM orderDetails;
&quot;&quot;&quot;, conn)

# Perform math operations
pd.read_sql(&quot;&quot;&quot;
SELECT priceEach * quantityOrdered AS total_price
  FROM orderDetails;
&quot;&quot;&quot;, conn)

# Split date strings into sub-parts
pd.read_sql(&quot;&quot;&quot;
SELECT orderDate,
       strftime(&quot;%m&quot;, orderDate) AS month,
       strftime(&quot;%Y&quot;, orderDate) AS year,
       strftime(&quot;%d&quot;, orderDate) AS day
  FROM orders;
&quot;&quot;&quot;, conn)

# Using WHERE
pd.read_sql(&quot;&quot;&quot;
SELECT *
  FROM employees
 WHERE lastName = &quot;Patterson&quot;;
&quot;&quot;&quot;, conn)

# Based on string conditions
pd.read_sql(&quot;&quot;&quot;
SELECT *, length(firstName) AS name_length
  FROM employees
 WHERE name_length = 5;
&quot;&quot;&quot;, conn)

# Based on value, rounded
pd.read_sql(&quot;&quot;&quot;
SELECT *, CAST(round(priceEach) AS INTEGER) AS rounded_price_int
  FROM orderDetails
 WHERE rounded_price_int = 30;
&quot;&quot;&quot;, conn)

# Select all columns for planets that have at least one moon and a mass less than 1.00
pd.read_sql(&quot;&quot;&quot;
SELECT *
  FROM planets
 WHERE num_of_moons &gt;= 1
   AND mass &lt; 1.00;
&quot;&quot;&quot;, conn)

# Select the name and color of planets that have a color containing the string &quot;blue&quot;
pd.read_sql(&quot;&quot;&quot;
SELECT name, color
  FROM planets
 WHERE color LIKE &quot;%blue%&quot;;
&quot;&quot;&quot;, conn)

# Return 10 newest orders not been shipped, not been canceled
pd.read_sql(&quot;&quot;&quot;
SELECT *
  FROM orders
 WHERE shippedDate = &quot;&quot;
   AND status != &quot;Cancelled&quot;
 ORDER BY orderDate DESC
 LIMIT 10;
&quot;&quot;&quot;, conn)</code></pre>
<hr />
</div>
</div>
<div id="api-call-example" class="section level2">
<h2>Conducting an API Call and Analyzing Results</h2>
<div id="querying-the-initial-request" class="section level3">
<h3>Querying the initial request</h3>
<pre class="python"><code># Import the requests library
import requests

# Get this from the &quot;Manage App&quot; page. Make sure you set them
# back to None before pushing this to GitHub, since otherwise
# your credentials will be compromised
api_key = None

# These can be whatever you want! But the solution uses &quot;pizza&quot;
# and &quot;New York NY&quot; if you want to compare your work directly
term = &quot;pizza&quot;
location = &quot;New York NY&quot;

# Set up params for request
url = &quot;https://api.yelp.com/v3/businesses/search&quot;
headers = {
    &quot;Authorization&quot;: &quot;Bearer {}&quot;.format(api_key)
}
url_params = {
    &quot;term&quot;: term.replace(&quot; &quot;, &quot;+&quot;),
    &quot;location&quot;: location.replace(&quot; &quot;, &quot;+&quot;)
}

# Make the request using requests.get, passing in
# url, headers=headers, and params=url_params
response = requests.get(url, headers=headers, params=url_params)

# Confirm we got a 200 response
response

# Get the response body in JSON format
response_json = response.json()

# View the keys
response_json.keys()</code></pre>
</div>
<div id="extracting-data" class="section level3">
<h3>Extracting Data</h3>
<pre class="python"><code># Retrieve the value from response_json
businesses = response_json[&quot;businesses&quot;]

# View the first 2 records
businesses[:2]</code></pre>
</div>
<div id="preparing-data" class="section level3">
<h3>Preparing Data</h3>
<pre class="python"><code>def prepare_data(data_list):
    &quot;&quot;&quot;
    This function takes in a list of dictionaries and prepares it
    for analysis
    &quot;&quot;&quot;
    
    # Make a new list to hold results
    results = []
    
    for business_data in data_list:
    
        # Make a new dictionary to hold prepared data for this business
        prepared_data = {}
        
        # Extract name, review_count, rating, and price key-value pairs
        # from business_data and add to prepared_data
        # If a key is not present in business_data, add it to prepared_data
        # with an associated value of None
        for key in (&quot;name&quot;, &quot;review_count&quot;, &quot;rating&quot;, &quot;price&quot;):
            prepared_data[key] = business_data.get(key, None)
    
        # Parse and add latitude and longitude columns
        coordinates = business_data[&quot;coordinates&quot;]
        prepared_data[&quot;latitude&quot;] = coordinates[&quot;latitude&quot;]
        prepared_data[&quot;longitude&quot;] = coordinates[&quot;longitude&quot;]
        
        # Add to list if all values are present
        if all(prepared_data.values()):
            results.append(prepared_data)
    
    return results
    
# Test out function
prepared_businesses = prepare_data(businesses)
prepared_businesses[:5]</code></pre>
</div>
<div id="exploratory-analysis" class="section level3">
<h3>Exploratory Analysis</h3>
<pre class="python"><code>from collections import Counter
import matplotlib.pyplot as plt
%matplotlib inline

fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(16, 5))

# Plot distribution of number of reviews
all_review_counts = [x[&quot;review_count&quot;] for x in full_dataset]
ax1.hist(all_review_counts)
ax1.set_title(&quot;Review Count Distribution&quot;)
ax1.set_xlabel(&quot;Number of Reviews&quot;)
ax1.set_ylabel(&quot;Number of Businesses&quot;)

# Plot rating distribution
all_ratings = [x[&quot;rating&quot;] for x in full_dataset]
rating_counter = Counter(all_ratings)
rating_keys = sorted(rating_counter.keys())
ax2.bar(rating_keys, [rating_counter[key] for key in rating_keys])
ax2.set_title(&quot;Rating Distribution&quot;)
ax2.set_xlabel(&quot;Rating&quot;)
ax2.set_ylabel(&quot;Number of Businesses&quot;)

# Plot price distribution
all_prices = [x[&quot;price&quot;].replace(&quot;$&quot;, r&quot;\$&quot;) for x in full_dataset]
price_counter = Counter(all_prices)
price_keys = sorted(price_counter.keys())
ax3.bar(price_keys, [price_counter[key] for key in price_keys])
ax3.set_title(&quot;Price Distribution&quot;)
ax3.set_xlabel(&quot;Price Category&quot;)
ax3.set_ylabel(&quot;Number of Businesses&quot;);

# Plot rating distribution by price
higher_price = []
lower_price = []
for row in full_dataset:
    if row[&quot;price&quot;] == &quot;$&quot;:
        lower_price.append(row[&quot;rating&quot;])
    else:
        higher_price.append(row[&quot;rating&quot;])
        
fig, ax = plt.subplots()

ax.hist([higher_price, lower_price], label=[&quot;higher price&quot;, &quot;lower price&quot;], density=True)

ax.legend();</code></pre>
<hr />
</div>
</div>
<div id="sets" class="section level2">
<h2>Sets</h2>
<pre class="python"><code># Create Set A
A = {2,4,6,8,10}
&#39;Type A: {}, A: {}&#39;.format(type(A), A)
# Returns
# &quot;Type A: &lt;class &#39;set&#39;&gt;, A: {2, 4, 6, 8, 10}&quot;

# Creat a Universal Set
U = set(range(1,13))
&#39;Type U: {}, U: {}&#39;.format(type(U), U)
# Returns
# &quot;Type U: &lt;class &#39;set&#39;&gt;, U: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}&quot;</code></pre>
<div id="code-examples-to-explore-sets" class="section level3">
<h3>Code examples to explore sets</h3>
<p><span class="math inline">\(A \cap B\)</span>
Intersection of A and B
New set with elements common to both A and B - only the elements in both Set A and Set B
<code>A.intersection(B)</code>
<code>A &amp; B</code></p>
<p><span class="math inline">\(A \cup B\)</span>
Union of A and B
New set with all elements from both A and B
<code>A.union(B)</code>
<code>A | B</code></p>
<p>A^{c}
Absolute complement of Set A - all elements in the universe not in Set A
<code>U.difference(A)</code>
<code>U - A</code></p>
<p>(<span class="math inline">\(A \cup B\)</span>)^{c}
The complement of the union of A and B
<code>U.difference(A.union(B))</code>
<code>U - (A/B)</code></p>
<p>A B
Relative complement of B - the elements of A that are not in B
The difference A - B
<code>A.difference(B)</code>
<code>A - B</code></p>
<p>C (B A)
The elements in C not in (the elements of B not in A)
The difference between C and (the difference between B and A)
<code>C.difference(B.difference(A))</code>
<code>C - (B - A)</code></p>
<hr />
</div>
</div>
<div id="prob-mass-function" class="section level2">
<h2>The Probability Mass Function</h2>
<pre class="python"><code># Divide each class size value by the total number of classes
# to create a pandas Series of probability mass function values
actual_pmf = pd.Series([value/sum_class for value in size_and_count.values()])

# Display probabilities in a dataframe
pmf_df = pd.concat([sizes, actual_pmf], axis=1)
pmf_df.columns = [&quot;Class Size&quot;, &quot;Overall Probability&quot;]
pmf_df.style.hide_index()

# Plot PMF
pmf_df.plot.bar(x=&quot;Class Size&quot;, y=&quot;Overall Probability&quot;);

# Function to calculate an input value in PMF
# p_actual - probability of any input&#39;s outcome
def p_actual(x_i):

# for any class size value, divided by total number of classes
return size_and_count[x_i] / sum_class

# Return the probability of that outcome
p_actual(17) # 0.13513513513513514

# Calculate expected value or mean $E(X)$
mu = (sizes.apply(p_actual) * sizes).sum()
mu
# 32.472972972972975</code></pre>
<hr />
</div>
<div id="prob-density-function" class="section level2">
<h2>The Probability Density Function</h2>
<pre class="python"><code>#Code to create a PDF histogram, non-parametric Kernel Density Estimation plot,
# and parametric distribution fit plot

import scipy.stats as stats

# Create two vertical subplots sharing 15% and 85% of plot space
# sharex allows sharing of axes i.e. building multiple plots on same axes
fig, (ax, ax2) = plt.subplots(2, sharex=True,
                                gridspec_kw={&quot;height_ratios&quot;: (.15, .85)},
                                figsize = (10,8) )

sns.distplot(data.Height,
                hist=True, hist_kws={
                                    &quot;linewidth&quot;: 2,
                                    &quot;edgecolor&quot; :&#39;red&#39;,
                                    &quot;alpha&quot;: 0.4,
                                    &quot;color&quot;: &quot;w&quot;,
                                    &quot;label&quot;: &quot;Histogram&quot;,
                                    },
                kde=True, kde_kws = {&#39;linewidth&#39;: 3,
                                        &#39;color&#39;: &quot;blue&quot;,
                                        &quot;alpha&quot;: 0.7,
                                        &#39;label&#39;:&#39;Kernel Density Estimation Plot&#39;
                                        },
                fit= stats.norm, fit_kws = {&#39;color&#39; : &#39;green&#39;,
                                            &#39;label&#39; : &#39;parametric fit&#39;,
                                            &quot;alpha&quot;: 0.7,
                                            &#39;linewidth&#39;:3},
                ax=ax2)


ax2.set_title(&#39;Density Estimations&#39;)
sns.boxplot(x=data.Height, ax = ax,color = &#39;red&#39;)
ax.set_title(&#39;Box and Whiskers Plot&#39;)
ax2.set(ylim=(0, .08))
plt.ylim(0,0.11)
plt.legend();

# Plot histograms with overlapping variables
binsize = 10

male_df.Height.plot.hist(bins = binsize, density = True, alpha = 0.7, label =&quot;Male Height&quot;);

female_df.Height.plot.hist(bins = binsize, density = True, alpha = 0.7, label = &#39;Female Height&#39;);

plt.legend()
plt.show()

# Write a density function
def density(x):

    n, bins = np.histogram(x, 10, density=1)

    # Initialize numpy arrays with zeros to store interpolated values
    pdfx = np.zeros(n.size)
    pdfy = np.zeros(n.size)

    # Interpolate through histogram bins
    # identify middle point between two neighbouring bins, in terms of x        and y coords
    for k in range(n.size):
        pdfx[k] = 0.5*(bins[k]+bins[k+1])
        pdfy[k] = n[k]

    # plot the calculated curve
    return pdfx, pdfy

# Generate test data and test the function
np.random.seed(5)
mu, sigma = 0, 0.1 # mean and standard deviation
s = np.random.normal(mu, sigma, 100)
x,y = density(s)

# Plot the density function
plt.plot(x,y, label = &#39;test&#39;)
plt.legend();

# Plot overlapping histograms using `seaborn`
sns.distplot(male_df.Weight)
sns.distplot(female_df.Weight)
plt.title(&#39;Comparing Weights&#39;)
plt.show()</code></pre>
<hr />
</div>
<div id="other-distributions-distributions" class="section level2">
<h2>Other Distributions {distributions}</h2>
<pre class="python"><code># Write a function to calculate the **cumulative density function**

# &#39;lst&#39; = list of all possible values
# &#39;X&#39; = value we want to calculate

def calculate_cdf(lst, X):
    count = 0
    # for all values in the list &#39;lst&#39;, if the value is less than or equal to X,
    # add one to count
    for value in lst:
    if value &lt;= X:
    count += 1

    # calculate cumulative probability of X by dividing the count by the total number of
    # possible values in the list
    cum_prob = count / len(lst) # normalizing cumulative probabilities (as with pmfs)
    return round(cum_prob, 3)

# test data
test_lst = [1,2,3]
test_X = 2

calculate_cdf(test_lst, test_X)
# 0.667

# Create a **binomial distribution function**
# n = number of trials
# k = number of success/outcomes we want
# p = probability of outcome

def binom_distr(n,p,k):
    p_k = (factorial(n)/(factorial(k)*factorial(n-k)))*(p**k*(1-p)**(n-k))
    return p_k
    

# The **normal distribution**
mu, sigma = 0.5, 0.1
n = 1000
s = np.random.normal(mu, sigma, n)
sns.distplot(s);

# Calculate the normal Density function
density = 1/(sigma * np.sqrt(2 * np.pi)) * np.exp( - (bins - mu)**2 / (2 * sigma**2))</code></pre>
<hr />
</div>
<div id="clt" class="section level2">
<h2>Central Limit Theorem</h2>
<pre class="python"><code># Write a function that would include the code for generating combinations as above and
# also for identifying the mean for each sample.

def sample_means(sample_size, data):

    &quot;&quot;&quot;
    This function takes in population data as a dictionary along with a chosen sample size 
    to generate all possible combinations of given sample size. 
    The function calculates the mean of each sample and returns:
    a) a list of all combinations ( as tuples ) 
    b) a list of means for all sample
    &quot;&quot;&quot;

    n = sample_size

    # Calculate the mean of population
    mu = calculate_mu(data)
    #print (&quot;Mean of population is:&quot;, mu)

    # Generate all possible combinations using given sample size

    combs = list(itertools.combinations(data, n))
    print (&quot;Using&quot;, n, &quot;samples with a population of size, we can see&quot;, len(combs), &quot;possible combinations &quot;)
    
    # Calculate the mean weight (x_bar) for all the combinations (samples) using the given data
    x_bar_list = []

    # Calculate sample mean for all combinations
    for i in range(len(combs)):
        sum = 0

        for j in range(n):
            key = combs[i][j]
            val =data[str(combs[i][j])]
            sum += val

        x_bar = sum/n
        x_bar_list.append(x_bar)
    print (&quot;The mean of all sample means mu_x_hat is:&quot;, np.mean(x_bar_list))

    return combs, x_bar_list

n = 2 #Sample size

combs, means = sample_means(n, pumpkin_dict)

# Print the sample combinations with their means
for c in range(len(combs)):
    print (c+1, combs[c], means[c])</code></pre>
<pre class="python"><code># Write a function to print a frequency table to identify the probability of seeing a different mean value.

def calculate_probability(means):
    &#39;&#39;&#39;
    Input: a list of means (x_hats)
    Output: a list of probablitity of each mean value
    &#39;&#39;&#39;
    #Calculate the frequency of each mean value
    freq = Counter(means)

    prob = []
    # Calculate and append fequency of each mean value in the prob list. 
    for element in means:
        for key in freq.keys():
            if element == key:
                prob.append(str(freq[key])+&quot;/&quot;+str(len(means)))
    return prob
    
probs = calculate_probability(means)

# Print combinations with sample means and probability of each mean value
for c in range(len(combs)):
    print (c+1, combs[c], means[c], probs[c])</code></pre>
<pre class="python"><code># Calculate standard error
# Create empty lists for storing sample means, combinations and standard error for each iteration
means_list = []
combs_list = []
err_list = []
for n in (1, 2,3,4,5):
    # Calculate combinations, means and probabilities as earlier
    
    combs, means = sample_means(n, pumpkin_dict)

    combs_list.append(combs)
    means_list.append(means)

    # Calculate the standard error by dividing sample means with square root of sample size
    err = round(np.std(means)/np.sqrt(n), 2)
    err_list.append(err)</code></pre>
<pre class="python"><code># Write a function to input population and sample data to calculate the confidence intervals.

def conf_interval(pop, sample):
    &#39;&#39;&#39;
    Function input: population , sample 
    Function output: z-critical, Margin of error, Confidence interval
    &#39;&#39;&#39;
    sample_size = 500
    n = len(sample)
    x_hat = sample.mean()

    # Calculate the z-critical value using stats.norm.ppf()
    # Note that we use stats.norm.ppf(q = 0.975) to get the desired z-critical value 
    # instead of q = 0.95 because the distribution has two tails.
    z = stats.norm.ppf(q = .975)  #  z-critical value for 95% confidence

    #Calculate the population std from data
    pop_stdev = pop.std()

    # Calculate the margin of error using formula given above
    moe = z * (pop_stdev/math.sqrt(sample_size))

    # Calculate the confidence interval by applying margin of error to sample mean 
    # (mean - margin of error, mean+ margin of error)
    conf = (x_hat - moe, x_hat + moe)
    
    return z, moe, conf

# Call above function with sample and population 
z_critical, margin_of_error, confidence_interval = conf_interval(population_ages, sample)    </code></pre>
<pre class="python"><code># Calculate margin of error using t_critical and se
margin_of_error = t_critical * se

# Calculate the confidence interval using margin_of_error
confidence_interval = (sample_mean - margin_of_error,
                       sample_mean + margin_of_error)</code></pre>
<hr />
</div>
<div id="effect-size-stat-power" class="section level2">
<h2>Effect Size and Statistical Power</h2>
<pre class="python"><code># Generate a normal distribution for male heights 
male_height = scipy.stats.norm(male_mean, male_sd)

# Generate a normal distribution for female heights 
female_height = scipy.stats.norm(female_mean, female_sd)

# Write a function to compute Coden&#39;s d
def Cohen_d(group1, group2):

    # Compute Cohen&#39;s d.

    # group1: Series or NumPy array
    # group2: Series or NumPy array

    # returns a floating point number 

    diff = group1.mean() - group2.mean()

    n1, n2 = len(group1), len(group2)
    var1 = group1.var()
    var2 = group2.var()

    # Calculate the pooled threshold as shown earlier
    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)
    
    # Calculate Cohen&#39;s d statistic
    d = diff / np.sqrt(pooled_var)
    
    return d
  
# Run simulations to run independent t-tests
# Initialize array to store results
p = (np.empty(n_sim))
p.fill(np.nan)

#  Run a for loop for range of values in n_sim
for s in range(n_sim):

    control = np.random.normal(loc= control_mean, scale=control_sd, size=sample_size)
    experimental = np.random.normal(loc= experimental_mean, scale=experimental_sd, size=sample_size)
    t_test = stats.ttest_ind(control, experimental)
    p[s] = t_test[1]

# number of null hypothesis rejections
num_null_rejects = np.sum(p &lt; 0.05)
power = num_null_rejects/float(n_sim)</code></pre>
<hr />
</div>
<div id="ab-testing" class="section level2">
<h2>A/B Hypothesis Testing</h2>
<pre class="python"><code># Calculate the required sample size for alpha = 0.05

from statsmodels.stats.power import TTestIndPower, TTestPower
power_analysis = TTestIndPower()
mean_difference = 0.01 # -&gt; 1% difference in response rate
sd = 0.0475 # -&gt; from set up
effect_size = mean_difference / sd
power_analysis.solve_power(alpha=.05, effect_size=effect_size, power=.80, alternative=&#39;larger&#39;)
# 279.67

# Convert value into a binary variable for analysis
control = df[df.group==&#39;control&#39;].pivot(index=&#39;id&#39;, columns=&#39;action&#39;, values=&#39;count&#39;)
control = control.fillna(value=0)

experiment = df[df.group==&#39;experiment&#39;].pivot(index=&#39;id&#39;, columns=&#39;action&#39;, values=&#39;count&#39;)
experiment = experiment.fillna(value=0)

# Calculate expected values
control_rate = control.click.mean()
expected_experiment_clicks_under_null = control_rate * len(experiment)
print(expected_experiment_clicks_under_null)

# Calculate the number of standard deviations that the actual number of clicks was from this estimate
# and the z-score.

n = len(experiment)
p = control_rate
var = n * p * (1-p)
std = np.sqrt(var)
print(std)

actual_experiment_clicks = experiment.click.sum()
z_score = (actual_experiment_clicks - expected_experiment_clicks_under_null)/std
print(z_score)

# Calculate a p-value using the normal distribution based on this z-score.

import scipy.stats as stats
p_val = stats.norm.sf(z_score) #or 1 - stats.norm.cdf(z_score)
print(p_val)</code></pre>
<hr />
<p>Course notes from <a href="https://flatironschool.com/courses/data-science-bootcamp/">The Flatiron School</a>
Code examples from <a href="https://github.com/learn-co-curriculum">Learn.co Curriculum</a></p>
</div>
