---
title: 'A-B Testing Lab'
author: 'Rebecca Frost-Brewer'
date: '2022-05-01'
slug: []
categories: []
tags: []
---



<div id="website-ab-testing---lab" class="section level1">
<h1>Website A/B Testing - Lab</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>In this lab, you’ll get another chance to practice your skills at conducting a full A/B test analysis. It will also be a chance to practice your data exploration and processing skills! The scenario you’ll be investigating is data collected from the homepage of a music app page for audacity.</p>
<!--more-->
</div>
<div id="objectives" class="section level2">
<h2>Objectives</h2>
<p>You will be able to:
* Analyze the data from a website A/B test to draw relevant conclusions
* Explore and analyze web action data</p>
</div>
<div id="exploratory-analysis" class="section level2">
<h2>Exploratory Analysis</h2>
<p>Start by loading in the dataset stored in the file ‘homepage_actions.csv’. Then conduct an exploratory analysis to get familiar with the data.</p>
<p>This data is collected from a homepage, with a control group and experiment group, and whether or not they viewed
the page or clicked the page. It includes the timestamp of action as well as the participant id.</p>
<p>Of the 8,188 participants, 6,328 viewed and only 1,860 clicked</p>
<pre class="python"><code>eids = set(df[df.group==&#39;experiment&#39;][&#39;id&#39;].unique())
cids = set(df[df.group==&#39;control&#39;][&#39;id&#39;].unique())
print(&#39;Overlap of experiment and control groups: {}&#39;.format(len(eids&amp;cids)))</code></pre>
</div>
<div id="conduct-a-statistical-test" class="section level2">
<h2>Conduct a Statistical Test</h2>
<p>Conduct a statistical test to determine whether the experimental homepage was more effective than that of the control group.</p>
<pre class="python"><code>#Convert clicks into a binary variable on a user-by-user-basis
control = df[df.group==&#39;control&#39;].pivot(index=&#39;id&#39;, columns=&#39;action&#39;, values=&#39;count&#39;)
control = control.fillna(value=0)

experiment = df[df.group==&#39;experiment&#39;].pivot(index=&#39;id&#39;, columns=&#39;action&#39;, values=&#39;count&#39;)
experiment = experiment.fillna(value=0)



print(&quot;Sample sizes:\tControl: {}\tExperiment: {}&quot;.format(len(control), len(experiment)))
# Sample sizes: Control: 3332   Experiment: 2996

print(&quot;Total Clicks:\tControl: {}\tExperiment: {}&quot;.format(control.click.sum(), experiment.click.sum()))
# Total Clicks: Control: 932.0  Experiment: 928.0

print(&quot;Average click rate:\tControl: {}\tExperiment: {}&quot;.format(control.click.mean(), experiment.click.mean()))
# Average click rate:   Control: 0.2797118847539016 Experiment: 0.3097463284379172

import flatiron_stats as fs
fs.p_value_welch_ttest(control.click, experiment.click)
# 0.004466402814337078</code></pre>
<div id="analysis" class="section level3">
<h3>Analysis:</h3>
<p>Does this result roughly match that of the previous statistical test?</p>
<blockquote>
<p>Comment: Yes, both p-values are &lt; 0.05, so we would reject the null hypothesis, indicating the the
experimental page has a more effective design than does the control page.</p>
</blockquote>
<p>Attribution: <a href="https://github.com/learn-co-curriculum/dsc-website-ab-testing-lab">The Flatiron School</a></p>
</div>
</div>
</div>
