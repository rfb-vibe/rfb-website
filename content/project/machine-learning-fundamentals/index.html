---
title: 'Machine Learning Theory and Fundamentals'
author: 'Rebecca Frost-Brewer'
date: '2022-06-07'
slug: machine-learning-fundamentals
categories: []
tags: []
projects: []
---



<div id="machine-learning-theory-and-fundamentals" class="section level1">
<h1>Machine Learning Theory and Fundamentals</h1>
<p>This lab works through an end-to-end machine learning workflow, focusing on the fundamental concepts of machine learning theory and processes. The main emphasis is on modeling theory (not EDA or preprocessing).</p>
<!--more-->
<div id="the-task-build-a-model-to-predict-blood-pressure" class="section level2">
<h2>The Task: Build a Model to Predict Blood Pressure</h2>
<div id="business-and-data-understanding" class="section level3">
<h3>Business and Data Understanding</h3>
<p>Hypertension (high blood pressure) is a treatable condition, but measuring blood pressure requires specialized equipment that most people do not have at home.</p>
<p>The question, then, is <strong><em>can we predict blood pressure using just a scale and a tape measure</em></strong>? These measuring tools, which individuals are more likely to have at home, might be able to flag individuals with an increased risk of hypertension.</p>
<p><a href="https://doi.org/10.1155/2014/637635">Researchers in Brazil</a> collected data from several hundred college students in order to answer this question. We will be specifically using the data they collected from female students.</p>
<p>The measurements we have are:</p>
<ul>
<li>Age (age in years)</li>
<li>BMI (body mass index, a ratio of weight to height)</li>
<li>WC (waist circumference in centimeters)</li>
<li>HC (hip circumference in centimeters)</li>
<li>WHR (waist-hip ratio)</li>
<li>SBP (systolic blood pressure)</li>
</ul>
</div>
</div>
<div id="perform-a-train-test-split" class="section level2">
<h2>1. Perform a Train-Test Split</h2>
<p>A machine learning (predictive) workflow fundamentally emphasizes creating <em>a model that will perform well on unseen data</em>. We will hold out a subset of our original data as the “test” set that will stand in for truly unseen data that the model will encounter in the future.</p>
<p>We make this separation as the first step for two reasons:</p>
<ol style="list-style-type: decimal">
<li>Most importantly, we are avoiding <em>leakage</em> of information from the test set into the training set. Leakage can lead to inflated metrics, since the model has information about the “unseen” data that it won’t have about real unseen data. This is why we always want to fit our transformers and models on the training data only, not the full dataset.</li>
<li>Also, we want to make sure the code we have written will actually work on unseen data. If we are able to transform our test data and evaluate it with our final model, that’s a good sign that the same process will work for future data as well.</li>
</ol>
<div id="loading-the-data" class="section level3">
<h3>Loading the Data</h3>
<p>In the cell below, we import the pandas library and open the full dataset for you. It has already been formatted and subsetted down to the relevant columns.</p>
<pre class="python"><code># Run this cell without changes
import pandas as pd
df = pd.read_csv(&quot;data/blood_pressure.csv&quot;, index_col=0)
df</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Age
</th>
<th>
bmi
</th>
<th>
wc
</th>
<th>
hc
</th>
<th>
whr
</th>
<th>
SBP
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
31
</td>
<td>
28.76
</td>
<td>
88
</td>
<td>
101
</td>
<td>
87
</td>
<td>
128.00
</td>
</tr>
<tr>
<th>
1
</th>
<td>
21
</td>
<td>
27.59
</td>
<td>
86
</td>
<td>
110
</td>
<td>
78
</td>
<td>
123.33
</td>
</tr>
<tr>
<th>
2
</th>
<td>
23
</td>
<td>
22.45
</td>
<td>
72
</td>
<td>
104
</td>
<td>
69
</td>
<td>
90.00
</td>
</tr>
<tr>
<th>
3
</th>
<td>
24
</td>
<td>
28.16
</td>
<td>
89
</td>
<td>
108
</td>
<td>
82
</td>
<td>
126.67
</td>
</tr>
<tr>
<th>
4
</th>
<td>
20
</td>
<td>
25.05
</td>
<td>
81
</td>
<td>
108
</td>
<td>
75
</td>
<td>
120.00
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
219
</th>
<td>
21
</td>
<td>
45.15
</td>
<td>
112
</td>
<td>
132
</td>
<td>
85
</td>
<td>
157.00
</td>
</tr>
<tr>
<th>
220
</th>
<td>
24
</td>
<td>
37.89
</td>
<td>
96
</td>
<td>
124
</td>
<td>
77
</td>
<td>
124.67
</td>
</tr>
<tr>
<th>
221
</th>
<td>
37
</td>
<td>
33.24
</td>
<td>
104
</td>
<td>
108
</td>
<td>
96
</td>
<td>
126.67
</td>
</tr>
<tr>
<th>
222
</th>
<td>
28
</td>
<td>
35.68
</td>
<td>
103
</td>
<td>
130
</td>
<td>
79
</td>
<td>
114.67
</td>
</tr>
<tr>
<th>
223
</th>
<td>
18
</td>
<td>
36.24
</td>
<td>
113
</td>
<td>
128
</td>
<td>
88
</td>
<td>
119.67
</td>
</tr>
</tbody>
</table>
<p>
224 rows × 6 columns
</p>
</div>
</div>
<div id="identifying-features-and-target" class="section level3">
<h3>Identifying Features and Target</h3>
<p>Once the data is loaded into a pandas dataframe, the next step is identifying which columns represent features and which column represents the target.</p>
<p>Recall that in this instance, we are trying to predict systolic blood pressure.</p>
<p>In the cell below, assign <code>X</code> to be the features and <code>y</code> to be the target. Remember that <code>X</code> should <strong>NOT</strong> contain the target.</p>
<pre class="python"><code># Replace None with appropriate code

X = df.drop(columns = &#39;SBP&#39;, axis = 1)
y = df[&#39;SBP&#39;]

X</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Age
</th>
<th>
bmi
</th>
<th>
wc
</th>
<th>
hc
</th>
<th>
whr
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
31
</td>
<td>
28.76
</td>
<td>
88
</td>
<td>
101
</td>
<td>
87
</td>
</tr>
<tr>
<th>
1
</th>
<td>
21
</td>
<td>
27.59
</td>
<td>
86
</td>
<td>
110
</td>
<td>
78
</td>
</tr>
<tr>
<th>
2
</th>
<td>
23
</td>
<td>
22.45
</td>
<td>
72
</td>
<td>
104
</td>
<td>
69
</td>
</tr>
<tr>
<th>
3
</th>
<td>
24
</td>
<td>
28.16
</td>
<td>
89
</td>
<td>
108
</td>
<td>
82
</td>
</tr>
<tr>
<th>
4
</th>
<td>
20
</td>
<td>
25.05
</td>
<td>
81
</td>
<td>
108
</td>
<td>
75
</td>
</tr>
<tr>
<th>
…
</th>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
<td>
…
</td>
</tr>
<tr>
<th>
219
</th>
<td>
21
</td>
<td>
45.15
</td>
<td>
112
</td>
<td>
132
</td>
<td>
85
</td>
</tr>
<tr>
<th>
220
</th>
<td>
24
</td>
<td>
37.89
</td>
<td>
96
</td>
<td>
124
</td>
<td>
77
</td>
</tr>
<tr>
<th>
221
</th>
<td>
37
</td>
<td>
33.24
</td>
<td>
104
</td>
<td>
108
</td>
<td>
96
</td>
</tr>
<tr>
<th>
222
</th>
<td>
28
</td>
<td>
35.68
</td>
<td>
103
</td>
<td>
130
</td>
<td>
79
</td>
</tr>
<tr>
<th>
223
</th>
<td>
18
</td>
<td>
36.24
</td>
<td>
113
</td>
<td>
128
</td>
<td>
88
</td>
</tr>
</tbody>
</table>
<p>
224 rows × 5 columns
</p>
</div>
<p>Make sure the assert statements pass before moving on to the next step:</p>
<pre class="python"><code># Run this cell without changes

# X should be a 2D matrix with 224 rows and 5 columns
assert X.shape == (224, 5)

# y should be a 1D array with 224 values
assert y.shape == (224,)</code></pre>
</div>
<div id="performing-train-test-split" class="section level3">
<h3>Performing Train-Test Split</h3>
<p>In the cell below, import <code>train_test_split</code> from scikit-learn (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">documentation here</a>).</p>
<p>Then create variables <code>X_train</code>, <code>X_test</code>, <code>y_train</code>, and <code>y_test</code> using <code>train_test_split</code> with <code>X</code>, <code>y</code>, and <code>random_state=2021</code>.</p>
<pre class="python"><code># Replace None with appropriate code

# Import the relevant function
from sklearn.model_selection import train_test_split

# Create train and test data using random_state=2021
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 2021)</code></pre>
<p>Make sure that the assert statements pass:</p>
<pre class="python"><code># Run this cell without changes

assert X_train.shape == (168, 5)
assert X_test.shape == (56, 5)

assert y_train.shape == (168,)
assert y_test.shape == (56,)</code></pre>
</div>
</div>
<div id="build-and-evaluate-a-first-simple-model" class="section level2">
<h2>2. Build and Evaluate a First Simple Model</h2>
<p>For our baseline model (FSM), we’ll use a <code>LinearRegression</code> from scikit-learn (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">documentation here</a>).</p>
<div id="instantiating-the-model" class="section level3">
<h3>Instantiating the Model</h3>
<p>In the cell below, instantiate a <code>LinearRegression</code> model and assign it to the variable <code>baseline_model</code>.</p>
<pre class="python"><code># Replace None with appropriate code

# Import the relevant class
from sklearn.linear_model import LinearRegression

# Instantiate a linear regression model
baseline_model = LinearRegression()</code></pre>
<p>Make sure the assert passes:</p>
<pre class="python"><code># Run this cell without changes

# baseline_model should be a linear regression model
assert type(baseline_model) == LinearRegression</code></pre>
<p>If you are getting the type of <code>baseline_model</code> as <code>abc.ABCMeta</code>, make sure you actually invoked the constructor of the linear regression class with <code>()</code>.</p>
<p>If you are getting <code>NameError: name 'LinearRegression' is not defined</code>, make sure you have the correct import statement.</p>
</div>
<div id="fitting-and-evaluating-the-model-on-the-full-training-set" class="section level3">
<h3>Fitting and Evaluating the Model on the Full Training Set</h3>
<p>In the cell below, fit the model on <code>X_train</code> and <code>y_train</code>:</p>
<pre class="python"><code># Your code here

baseline_model.fit(X_train, y_train)</code></pre>
<p>LinearRegression()</p>
<p>Then, evaluate the model using root mean squared error (RMSE). To do this, first import the <code>mean_squared_error</code> function from scikit-learn (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html">documentation here</a>). Then pass in both the actual and predicted y values, along with <code>squared=False</code> (to get the RMSE rather than MSE).</p>
<pre class="python"><code># Replace None with appropriate code

# Import the relevant function
from sklearn.metrics import mean_squared_error

# Generate predictions using baseline_model and X_train
y_pred_baseline = baseline_model.predict(X_train)

# Evaluate using mean_squared_error with squared=False

baseline_rmse = mean_squared_error(y_train, baseline_model.predict(X_train), squared = False)
baseline_rmse</code></pre>
<p>15.97633456376879</p>
<p>Your RMSE calculation should be around 15.98:</p>
<pre class="python"><code># Run this cell without changes
assert round(baseline_rmse, 2) == 15.98</code></pre>
<p>This means that on the <em>training</em> data, our predictions are off by about 16 mmHg on average.</p>
<p>But what about on <em>unseen</em> data?</p>
<p>To stand in for true unseen data (and avoid making decisions based on this particular data split, therefore not using <code>X_test</code> or <code>y_test</code> yet), let’s use cross-validation.</p>
</div>
<div id="fitting-and-evaluating-the-model-with-cross-validation" class="section level3">
<h3>Fitting and Evaluating the Model with Cross Validation</h3>
<p>In the cell below, import <code>cross_val_score</code> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html">documentation here</a>) and call it with <code>baseline_model</code>, <code>X_train</code>, and <code>y_train</code>.</p>
<p>For specific implementation reasons within the scikit-learn library, you’ll need to use <code>scoring="neg_root_mean_squared_error"</code>, which returns the RMSE values with their signs flipped to negative. Then we take the average and negate it at the end, so the number is directly comparable to the RMSE number above.</p>
<pre class="python"><code># Replace None with appropriate code

# Import the relevant function
from sklearn.model_selection import cross_val_score

# Get the cross validated scores for our baseline model
baseline_cv = cross_val_score(baseline_model, X_train, y_train, scoring = &#39;neg_root_mean_squared_error&#39;)

# Display the average of the cross-validated scores
baseline_cv_rmse = -(baseline_cv.mean())
baseline_cv_rmse</code></pre>
<p>15.953844849875598</p>
<p>The averaged RMSE for the cross-validated scores should be around 15.95:</p>
<pre class="python"><code># Run this cell without changes

assert round(baseline_cv_rmse, 2) == 15.95</code></pre>
</div>
<div id="analysis-of-baseline-model" class="section level3">
<h3>Analysis of Baseline Model</h3>
<p>So, we got an RMSE of about 16 for both the training data and the validation data. RMSE is a form of <em>error</em>, so this means the performance is somewhat better on the validation data than the training data. (This is a bit unusual — normally we expect to see better scores on the training data, but maybe there are some outliers or other reasons that this particular split has this result.)</p>
<p>Referring back to the chart above, both errors mean that on average we would expect to mix up someone with stage 1 vs. stage 2 hypertension, but not someone with normal blood pressure vs. critical hypertension. So it appears that the features we have might be predictive enough to be useful.</p>
<p>Are we overfitting? Underfitting?</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>The RMSE values for the training data and test data are fairly close to each other and the validation score is actually slightly better than the training score, so we can assume that we are not overfitting.</p>
<p>It seems like our model has some room for improvement, but without further investigation it’s impossible to know whether we are underfitting, or there is just irreducible error present. Maybe we are simply missing the features we would need to reduce error. (For example, we don’t know anything about the diets of these study participants, and we know that diet can influence blood pressure.) But it’s also possible that there is some reducible error, meaning we are currently underfitting.</p>
<p>In the next step, we’ll assume we <em>are</em> underfitting, and will attempt to reduce that underfitting by applying some polynomial features transformations to the data.</p>
</div>
</div>
<div id="use-polynomialfeatures-to-reduce-underfitting" class="section level2">
<h2>3. Use <code>PolynomialFeatures</code> to Reduce Underfitting</h2>
<p>Comprehension check: does “underfitting” mean we have high <em>bias</em>, or high <em>variance</em>?</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>Underfitting means high bias. While it’s possible that your model will have both high bias and high variance at the same time, in general underfitting means that there is additional information in the data that your model currently isn’t picking up on, so you are getting higher error metrics than necessary.</p>
<p>In some model algorithms (e.g. k-nearest neighbors) there are hyperparameters we can adjust so that the model is more flexible and can pick up on additional information in the data. In this case, since we are using linear regression, let’s instead perform some feature engineering with <code>PolynomialFeatures</code>.</p>
<div id="creating-polynomialfeatures-transformer-fitting-and-transforming-x_train" class="section level3">
<h3>Creating <code>PolynomialFeatures</code> Transformer, Fitting and Transforming <code>X_train</code></h3>
<p>In the cell below, instantiate a <code>PolynomialFeatures</code> transformer with default arguments (i.e. just <code>PolynomialFeatures()</code>). Documentation for <code>PolynomialFeatures</code> can be found <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html">here</a>.</p>
<p>Then fit the transformer on <code>X_train</code> and create a new <code>X_train_poly</code> matrix by transforming <code>X_train</code>.</p>
<pre class="python"><code># Replace None with appropriate code

# Import the relevant class
from sklearn.preprocessing import PolynomialFeatures

# Instantiate polynomial features transformer
poly = PolynomialFeatures()

# Fit transformer on entire X_train
X_train_poly = poly.fit_transform(X_train)

# Create transformed data matrix by transforming X_train
X_train_poly = pd.DataFrame(X_train_poly)</code></pre>
<p>Check that <code>poly</code> was instantiated correctly, and <code>X_train_poly</code> has the correct shape:</p>
<pre class="python"><code># Run this cell without changes

assert type(poly) == PolynomialFeatures

assert X_train_poly.shape == (168, 21)</code></pre>
</div>
<div id="fitting-and-evaluating-the-model-on-the-transformed-training-set" class="section level3">
<h3>Fitting and Evaluating the Model on the Transformed Training Set</h3>
<p>In the cell below, fit the <code>baseline_model</code> on <code>X_train_poly</code> and <code>y_train</code>, then find the RMSE using the same technique you used in Step 2.</p>
<pre class="python"><code># Replace None with appropriate code

# Fit baseline_model
poly_lr = LinearRegression()
poly_lr.fit(X_train_poly, y_train)

# Make predictions
y_pred_poly = poly_lr.predict(X_train_poly)

# Find the RMSE on the full X_train_poly and y_train
poly_rmse = mean_squared_error(y_train, poly_lr.predict(X_train_poly), squared = False)
poly_rmse</code></pre>
<p>15.070011404625243</p>
<p>The new RMSE should be about 15.07:</p>
<pre class="python"><code># Run this cell without changes

assert round(poly_rmse, 2) == 15.07</code></pre>
</div>
<div id="fitting-and-evaluating-the-model-with-cross-validation-1" class="section level3">
<h3>Fitting and Evaluating the Model with Cross Validation</h3>
<p>In the cell below, use <code>cross_val_score</code> to find an averaged cross-validated RMSE using the same technique you used in Step 2.</p>
<pre class="python"><code># Replace None with appropriate code

# Get the cross validated scores for our transformed features
poly_cv = cross_val_score(poly_lr, X_train_poly, y_train, scoring = &#39;neg_root_mean_squared_error&#39;)

# Display the average of the cross-validated scores
poly_cv_rmse = -(poly_cv.mean())
poly_cv_rmse</code></pre>
<p>17.738421720231184</p>
<p>The cross-validated RMSE should be about 17.74:</p>
<pre class="python"><code># Run this cell without changes

assert round(poly_cv_rmse, 2) == 17.74</code></pre>
</div>
<div id="analysis-of-polynomialfeatures-transformation" class="section level3">
<h3>Analysis of <code>PolynomialFeatures</code> Transformation</h3>
<p>The cell below displays the baseline and transformed values for the full training set vs. the cross-validated average:</p>
<pre class="python"><code># Run this cell without changes

print(&quot;Baseline Model&quot;)
print(&quot;Train RMSE:&quot;, baseline_rmse)
print(&quot;Validation RMSE:&quot;, baseline_cv_rmse)
print()
print(&quot;Model with Polynomial Transformation&quot;)
print(&quot;Train RMSE:&quot;, poly_rmse)
print(&quot;Validation RMSE:&quot;, poly_cv_rmse)</code></pre>
<p>Baseline Model
Train RMSE: 15.97633456376879
Validation RMSE: 15.953844849875598</p>
<p>Model with Polynomial Transformation
Train RMSE: 15.070011404625243
Validation RMSE: 17.738421720231184</p>
<p>So, what does this mean about the result of our polynomial features transformation? What was the impact on bias (underfitting)? What was the impact on variance (overfitting)?</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>The polynomial features transformation did successfully reduce bias (reduce underfitting). We can tell because the RMSE decreased on the training dataset. However, it also increased variance (increased overfitting). We can tell because the RMSE increased on the validation dataset compared to the train dataset.</p>
<p>Essentially this means that the polynomial features transformation gave our model the ability to pick up on more information from the training dataset, but some of that information was actually “noise” and not information that was useful for making predictions on unseen data.</p>
<p>In the cell below, we plot the train vs. validation RMSE across various different degrees of <code>PolynomialFeatures</code>:</p>
<pre class="python"><code># Run this cell without changes

# Create lists of RMSE values
train_rmse = []
val_rmse = []

# Create list of degrees we want to consider
degrees = list(range(1,8))

for degree in degrees:
    # Create transformer of relevant degree and transform X_train
    poly = PolynomialFeatures(degree)
    X_train_poly = poly.fit_transform(X_train)
    baseline_model.fit(X_train_poly, y_train)
    
    # RMSE for training data
    y_pred_poly = baseline_model.predict(X_train_poly)
    train_rmse.append(mean_squared_error(y_train, y_pred_poly, squared=False))
    
    # RMSE for validation data
    poly_cv = cross_val_score(baseline_model, X_train_poly, y_train, scoring=&quot;neg_root_mean_squared_error&quot;)
    val_rmse.append(-(poly_cv.mean()))

# Set up plot
import matplotlib.pyplot as plt
fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(13,5))

# Plot RMSE for training data
ax1.plot(degrees, train_rmse)
ax1.set_title(&quot;Training Data&quot;)

# Plot RMSE for validation data
ax2.plot(degrees, val_rmse, color=&quot;orange&quot;)
ax2.set_title(&quot;Validation Data&quot;)

# Shared attributes for plots
for ax in (ax1, ax2):
    ax.set_xticks(degrees)
    ax.set_xlabel(&quot;Polynomial Degree&quot;)
    ax.set_ylabel(&quot;RMSE&quot;)</code></pre>
<div class="figure">
<img src="output_45_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>Based on the above graphs, let’s plan to use a polynomial degree of 5. Why? Because that is where the RMSE for the training data has dropped down to essentially zero, meaning we are close to perfectly overfitting on the training data.</p>
<p>(This is a design decision where there isn’t always a single right answer. Later we will introduce a tool called “grid search” that will allow you to tune multiple aspects of the model at once instead of having to choose one step at a time like this.)</p>
<pre class="python"><code># Run this cell without changes

# Create transformer of relevant degree and transform X_train
poly = PolynomialFeatures(5)
X_train_poly = poly.fit_transform(X_train)
baseline_model.fit(X_train_poly, y_train)

# RMSE for training data
y_pred_poly = baseline_model.predict(X_train_poly)
final_poly_rmse = mean_squared_error(y_train, y_pred_poly, squared=False)

# RMSE for validation data
poly_cv = cross_val_score(baseline_model, X_train_poly, y_train, scoring=&quot;neg_root_mean_squared_error&quot;)
final_poly_cv_rmse = -(poly_cv.mean())</code></pre>
<pre class="python"><code># Run this cell without changes

print(&quot;Baseline Model&quot;)
print(&quot;Train RMSE:&quot;, baseline_rmse)
print(&quot;Validation RMSE:&quot;, baseline_cv_rmse)
print()
print(&quot;Model with Polynomial Transformation (Degree 5)&quot;)
print(&quot;Train RMSE:&quot;, final_poly_rmse)
print(&quot;Validation RMSE:&quot;, final_poly_cv_rmse)</code></pre>
<p>Baseline Model
Train RMSE: 15.97633456376879
Validation RMSE: 15.953844849875598</p>
<p>Model with Polynomial Transformation (Degree 5)
Train RMSE: 1.747660277993021e-06
Validation RMSE: 17109.831108490067</p>
<p>We have a dramatically improved train RMSE (approximately 16 down to 0) and a dramatically worsened validation RMSE (approximately 16 up to 17,000). At this point we are clearly overfitting, but we have successfully reduced the underfitting on the training dataset.</p>
<p>In the next step, let’s apply a technique to address this overfitting.</p>
</div>
</div>
<div id="use-regularization-to-reduce-overfitting" class="section level2">
<h2>4. Use Regularization to Reduce Overfitting</h2>
<p>Let’s use regularization to address this overfitting, specifically using the <code>Ridge</code> model from scikit-learn (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html">documentation here</a>), which uses the L2 norm.</p>
<div id="scaling-the-data" class="section level3">
<h3>Scaling the Data</h3>
<p>Because L2 regularization is distance-based, we need to scale our data before passing it into this model. In the cell below, instantiate a <code>StandardScaler</code> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html">documentation here</a>) and fit then transform the full <code>X_train_poly</code>.</p>
<pre class="python"><code># Replace None with appropriate code

# Import the relevant class
from sklearn.preprocessing import StandardScaler

# Instantiate the scaler
scaler = StandardScaler()

# Fit the scaler on X_train_poly
# Transform the data and create a new matrix
X_train_scaled = scaler.fit_transform(X_train_poly)</code></pre>
<p>The scaled data should have the same shape as <code>X_train_poly</code> but the values should be different:</p>
<pre class="python"><code># Run this cell without changes

assert X_train_scaled.shape == X_train_poly.shape
assert X_train_scaled[0][0] != X_train_poly[0][0]</code></pre>
</div>
<div id="fitting-a-ridge-model" class="section level3">
<h3>Fitting a Ridge Model</h3>
<p>In the cell below, instantiate a <code>Ridge</code> model with <code>random_state=42</code>, then fit it on <code>X_train_scaled</code> and <code>y_train</code>.</p>
<pre class="python"><code># Replace None with appropriate code

# Import the relevant class
from sklearn.linear_model import Ridge

# Instantiate the model with random_state=42
ridge_model = Ridge(random_state = 42)

# Fit the model
ridge_model.fit(X_train_scaled, y_train)</code></pre>
<p>Ridge(random_state=42)</p>
</div>
<div id="metrics-for-ridge-model" class="section level3">
<h3>Metrics for Ridge Model</h3>
<p>Now, find the train and cross-validated RMSE values, and assign them to <code>ridge_rmse</code> and <code>ridge_cv_rmse</code> respectively. You can refer back to previous steps to remember how to do this! Remember to use <code>ridge_model</code> and <code>X_train_scaled</code>.</p>
<pre class="python"><code># Your code here

# RMSE for training data
y_pred_ridge = ridge_model.predict(X_train_scaled)
ridge_rmse = mean_squared_error(y_train, y_pred_ridge, squared=False)

# RMSE for validation data
ridge_cv = cross_val_score(ridge_model, X_train_scaled, y_train, scoring=&quot;neg_root_mean_squared_error&quot;)
ridge_cv_rmse = -(ridge_cv.mean())

print(&quot;Train RMSE:&quot;, ridge_rmse)
print(&quot;Validation RMSE:&quot;, ridge_cv_rmse)</code></pre>
<p>Train RMSE: 15.23990847052503
Validation RMSE: 16.053219757460575</p>
<p>Your train RMSE should be about 15.24, and validation RMSE should be about 16.05:</p>
<pre class="python"><code># Run this cell without changes

assert round(ridge_rmse, 2) == 15.24
assert round(ridge_cv_rmse, 2) == 16.05</code></pre>
</div>
<div id="analysis-of-model-with-regularization" class="section level3">
<h3>Analysis of Model with Regularization</h3>
<p>The following cell shows metrics for each model so far:</p>
<pre class="python"><code># Run this cell without changes

print(&quot;Baseline Model&quot;)
print(&quot;Train RMSE:&quot;, baseline_rmse)
print(&quot;Validation RMSE:&quot;, baseline_cv_rmse)
print()
print(&quot;Model with Polynomial Transformation (Degree 5)&quot;)
print(&quot;Train RMSE:&quot;, final_poly_rmse)
print(&quot;Validation RMSE:&quot;, final_poly_cv_rmse)
print()
print(&quot;Model with Polynomial Transformation + Regularization&quot;)
print(&quot;Train RMSE:&quot;, ridge_rmse)
print(&quot;Validation RMSE:&quot;, ridge_cv_rmse)</code></pre>
<p>Baseline Model
Train RMSE: 15.97633456376879
Validation RMSE: 15.953844849875598</p>
<p>Model with Polynomial Transformation (Degree 5)
Train RMSE: 1.747660277993021e-06
Validation RMSE: 17109.831108490067</p>
<p>Model with Polynomial Transformation + Regularization
Train RMSE: 15.23990847052503
Validation RMSE: 16.053219757460575</p>
<p>Did we successfully reduce overfitting? Which model is the best model so far?</p>
<p>.</p>
<p>.</p>
<p>.</p>
<p>Compared to the model with the polynomial transformation, yes, we successfully reduced overfitting. We can tell because the gap between the train and validation RMSE got a lot smaller.</p>
<p>At this point, our best model is actually still the baseline model. Even though we have a lower RMSE for the training data with both the model with polynomial transformation and the model with regularization added, the validation RMSE was still lowest for the baseline model.</p>
<p>Let’s try adding stronger regularization penalties, to see if we can reduce the overfitting a bit further while still keeping the improvements to underfitting that we got from the polynomial features transformation.</p>
<pre class="python"><code># Run this cell without changes

# Create lists of RMSE values
train_rmse = []
val_rmse = []

# Create list of alphas we want to consider
alphas = [1, 10, 25, 50, 75, 100, 125, 250, 500]

for alpha in alphas:
    # Fit a model with a given regularization penalty
    model = Ridge(random_state=42, alpha=alpha)
    model.fit(X_train_scaled, y_train)
    
    # RMSE for training data
    y_pred_ridge = model.predict(X_train_scaled)
    train_rmse.append(mean_squared_error(y_train, y_pred_ridge, squared=False))
    
    # RMSE for validation data
    ridge_cv = cross_val_score(model, X_train_scaled, y_train, scoring=&quot;neg_root_mean_squared_error&quot;)
    val_rmse.append(-(ridge_cv.mean()))

# Plot train vs. validation RMSE
fig, ax = plt.subplots(figsize=(6,6))
ax.plot(alphas, train_rmse, label=&quot;Training Data&quot;)
ax.plot(alphas, val_rmse, label=&quot;Validation Data&quot;)
ax.set_xlabel(&quot;Alpha (Regularization Penalty)&quot;)
ax.set_ylabel(&quot;RMSE&quot;)
ax.legend();</code></pre>
<div class="figure">
<img src="output_63_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>(This time both are plotted on the same axes because the RMSE has the same order of magnitude.)</p>
<p>As we increase the alpha (regularization penalty) along the x-axis, first we can see a big drop in the validation RMSE, then as we keep penalizing more, eventually the RMSE for both the training and validation data starts increasing (meaning we are starting to underfit again).</p>
<p>The code below finds the best alpha value from our list, i.e. the alpha that results in the lowest RMSE for the validation data:</p>
<pre class="python"><code># Run this cell without changes

lowest_rmse = min(val_rmse)
print(&quot;Lowest RMSE:&quot;, lowest_rmse)

best_alpha = alphas[val_rmse.index(lowest_rmse)]
print(&quot;Best alpha:&quot;, best_alpha)</code></pre>
<p>Lowest RMSE: 15.674064966813983
Best alpha: 100</p>
<p>Let’s build a final model using that alpha value and compare it to our previous models:</p>
<pre class="python"><code># Run this cell without changes

# Fit a model with a given regularization penalty
final_model = Ridge(random_state=42, alpha=best_alpha)
final_model.fit(X_train_scaled, y_train)

# RMSE for training data
y_pred_final = final_model.predict(X_train_scaled)
final_rmse = mean_squared_error(y_train, y_pred_final, squared=False)

# RMSE for validation data
final_cv = cross_val_score(final_model, X_train_scaled, y_train, scoring=&quot;neg_root_mean_squared_error&quot;)
final_cv_rmse = -(final_cv.mean())</code></pre>
<pre class="python"><code># Run this cell without changes

print(&quot;Baseline Model&quot;)
print(&quot;Train RMSE:&quot;, baseline_rmse)
print(&quot;Validation RMSE:&quot;, baseline_cv_rmse)
print()
print(&quot;Model with Polynomial Transformation (Degree 5)&quot;)
print(&quot;Train RMSE:&quot;, final_poly_rmse)
print(&quot;Validation RMSE:&quot;, final_poly_cv_rmse)
print()
print(&quot;Final Model with Polynomial Transformation + Regularization&quot;)
print(&quot;Train RMSE:&quot;, final_rmse)
print(&quot;Validation RMSE:&quot;, final_cv_rmse)</code></pre>
<p>Baseline Model
Train RMSE: 15.97633456376879
Validation RMSE: 15.953844849875598</p>
<p>Model with Polynomial Transformation (Degree 5)
Train RMSE: 1.747660277993021e-06
Validation RMSE: 17109.831108490067</p>
<p>Final Model with Polynomial Transformation + Regularization
Train RMSE: 15.85687819199106
Validation RMSE: 15.674064966813983</p>
</div>
<div id="choosing-a-final-model" class="section level3">
<h3>Choosing a Final Model</h3>
<p>While we have already labeled a model as <code>final_model</code> above, make sure you understand why: this is the model with the best (lowest) validation RMSE. We also improved the train RMSE somewhat as well, meaning that our modeling strategy has actually reduced both underfitting and overfitting!</p>
<p>The impact of the changes made so far have been minimal, which makes sense given our business context. We are trying to predict blood pressure based on proxy measurements that leave out a lot of important information! But we still did see some improvement over the basline by applying polynomial feature transformation and regularization.</p>
</div>
</div>
<div id="evaluate-a-final-model-on-the-test-set" class="section level2">
<h2>5. Evaluate a Final Model on the Test Set</h2>
<p>Often our lessons leave out this step because we are focused on other concepts, but if you were to present your final model to stakeholders, it’s important to perform one final analysis on truly unseen data to make sure you have a clear idea of how the model will perform in the field.</p>
<div id="instantiating-the-final-model" class="section level3">
<h3>Instantiating the Final Model</h3>
<p>Unless you are using a model that is very slow to fit, it’s a good idea to re-create it from scratch prior to the final evaluation. That way you avoid any artifacts of how you iterated on the model previously.</p>
<p>In the cell below, instantiate a <code>Ridge</code> model with <code>random_state=42</code> and <code>alpha=100</code>.</p>
<pre class="python"><code># Replace None with appropriate code

final_model = Ridge(random_state=42, alpha=100)</code></pre>
</div>
<div id="fitting-the-final-model-on-the-training-data" class="section level3">
<h3>Fitting the Final Model on the Training Data</h3>
<p>You can go ahead and use the <code>X_train_scaled</code> and <code>y_train</code> data we created earlier.</p>
<pre class="python"><code># Your code here

final_model.fit(X_train_scaled, y_train)</code></pre>
<p>Ridge(alpha=100, random_state=42)</p>
</div>
<div id="preprocessing-the-test-set" class="section level3">
<h3>Preprocessing the Test Set</h3>
<p>The training data for our final model was transformed in two ways:</p>
<ol style="list-style-type: decimal">
<li>Polynomial features added by the <code>poly</code> transformer object</li>
<li>Scaled by the <code>scaler</code> transformer object</li>
</ol>
<p>In the cell below, transform the test data in the same way, with the same transformer objects. Do NOT re-instantiate or re-fit these objects.</p>
<pre class="python"><code># Replace None with appropriate code

# Add polynomial features
X_test_poly = poly.transform(X_test)

# Scale data
X_test_scaled = scaler.transform(X_test_poly)</code></pre>
<p>Make sure the shape is correct. If you have too few columns, make sure that you passed the transformed version of <code>X_test</code> (<code>X_test_poly</code>) to the scaler rather than just <code>X_test</code>.</p>
<pre class="python"><code># Run this cell without changes

assert X_test_scaled.shape == (56, 252)</code></pre>
</div>
<div id="evaluating-rmse-with-final-model-and-preprocessed-test-set" class="section level3">
<h3>Evaluating RMSE with Final Model and Preprocessed Test Set</h3>
<p>This time we don’t need to use cross-validation, since we are using the test set. In the cell below, generate predictions for the test data then use <code>mean_squared_error</code> with <code>squared=False</code> to find the RMSE for our holdout test set.</p>
<pre class="python"><code># Replace None with appropriate code

# Generate predictions
y_pred_test = final_model.predict(X_test_scaled)

# Find RMSE
test_rmse = mean_squared_error(y_test, y_pred_test, squared=False)
test_rmse
</code></pre>
<p>13.25652647434673</p>
</div>
<div id="interpreting-our-results" class="section level3">
<h3>Interpreting Our Results</h3>
<p>So, we successfully used polynomial features transformation and regularization to improve our metrics. But, can we recommend that this model be used for the purpose of predicting blood pressure based on these features?</p>
<p>Let’s create a scatter plot of actual vs. predicted blood pressure, with the boundaries of high blood pressure indicated:</p>
<pre class="python"><code># Run this cell without changes
import seaborn as sns

# Set up plot
fig, ax = plt.subplots(figsize=(8,6))

# Seaborn scatter plot with best fit line
sns.regplot(x=y_test, y=y_pred_test, ci=None, truncate=False, ax=ax)
ax.set_xlabel(&quot;Actual Blood Pressure&quot;)
ax.set_ylabel(&quot;Predicted Blood Pressure&quot;)

# Add spans showing high blood pressure + legend
ax.axvspan(129, max(y_test) + 1, alpha=0.2, color=&quot;blue&quot;, label=&quot;actual high blood pressure risk&quot;)
ax.axhspan(129, max(y_pred_test) + 1, alpha=0.2, color=&quot;gray&quot;, label=&quot;predicted high blood pressure risk&quot;)
ax.legend();</code></pre>
<div class="figure">
<img src="output_81_0.png" alt="" />
<p class="caption">png</p>
</div>
<p>In general, as the true blood pressure values increase, so do the predicted blood pressure values. So, it’s clear that our model is picking up on <em>some</em> information from our features.</p>
<p>But it looks like this model does not actually solve the initial business problem very well. Recall that our question was: <strong><em>can we predict blood pressure using just a scale and a tape measure?</em></strong> Our model would incorrectly flag one person as being at risk of high blood pressure, while missing all of the people who actually are at risk of high blood pressure.</p>
<p>It is possible that some other model algorithm (e.g. k-nearest neighbors or decision trees) would do a better job of picking up on the underlying patterns in this dataset. Or if we set this up as a classification problem rather than a regression problem, if we’re only interested in flagging high blood pressure rather than predicting blood pressure in general.</p>
<p>But if we had to stop this analysis now in its current state, we would need to conclude that <strong>while we were able to pick up some information about blood pressure using these variables alone, we did not produce a model that would work for this business case</strong>.</p>
<p>This is something that happens sometimes — not every target can be predicted with the features you have been given! In this case, maybe your model would still be useful for epidemiological modeling (predicting the blood pressure in populations) rather than predicting blood pressure for an individual, since we are picking up on some information. Further study would be needed to determine the feasibility of this approach.</p>
</div>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>In this cumulative lab, you performed an end-to-end machine learning process with correct usage of training, validation, and test data. You identified underfitting and overfitting and applied strategies to address them. Finally, you evaluated your final model using test data, and interpreted those results in the context of a business problem.</p>
</div>
</div>
