---
title: 'Linear Regression Analysis'
author: 'Rebecca Frost-Brewer'
date: '2022-05-01'
slug: []
categories: []
tags: []
---



<div id="linear-regression-analysis" class="section level1">
<h1>Linear Regression Analysis</h1>
<p>This project demonstrates a full linear regression analysis and reports the findings of a final model, including both predictive model performance metrics and interpretation of fitted model parameters.</p>
<!--more-->
<div id="business-understanding" class="section level2">
<h2>Business Understanding</h2>
<p>Develop a pricing algorithm to help set a target price for new LEGO sets that are released to market. The goal is to save the company some time and to help ensure consistency in pricing between new products and past products.</p>
<p>The main purpose of this algorithm is <em>predictive</em>, meaning that <strong>the model should be able to take in attributes of a LEGO set that does not yet have a set price, and to predict a good price</strong>.</p>
<p>The secondary purpose of this algorithm is <em>inferential</em>, meaning that <strong>the model should be able to tell us something about the relationship between the attributes of a LEGO set and its price</strong>.</p>
</div>
<div id="data-understanding" class="section level2">
<h2>Data Understanding</h2>
<p>The dataset contains over 700 LEGO sets released in the past, including attributes of those sets as well as their prices. The files have already been split into train and test sets.</p>
<pre class="python"><code>train = pd.read_csv(&quot;data/lego_train.csv&quot;)
test = pd.read_csv(&quot;data/lego_test.csv&quot;)

X_train = train.drop(&quot;list_price&quot;, axis=1)
y_train = train[&quot;list_price&quot;]

X_test = test.drop(&quot;list_price&quot;, axis=1)
y_test = test[&quot;list_price&quot;]

X_train</code></pre>
</div>
<div id="baseline-model" class="section level2">
<h2>Baseline Model</h2>
<pre class="python"><code>from sklearn.linear_model import LinearRegression

baseline_model = LinearRegression()

from sklearn.model_selection import cross_validate, ShuffleSplit

# Perform 3 separate train-test splits within the X_train and y_train sets
splitter = ShuffleSplit(n_splits=3, test_size=0.25, random_state=0)

# Find the train and test scores for each
baseline_scores = cross_validate(
    estimator=baseline_model,
    X=X_train[[most_correlated_feature]],
    y=y_train,
    return_train_score=True,
    cv=splitter
)

print(&quot;Train score:     &quot;, baseline_scores[&quot;train_score&quot;].mean())
# Train score:      0.7785726407224942

print(&quot;Validation score:&quot;, baseline_scores[&quot;test_score&quot;].mean())
# Validation score: 0.7793473618106956</code></pre>
</div>
<div id="build-a-model-with-all-numeric-features" class="section level2">
<h2>Build a Model with All Numeric Features</h2>
<div id="numeric-feature-selection" class="section level3">
<h3>Numeric Feature Selection</h3>
<p>Create a dataframe <code>X_train_numeric</code> that is a copy of <code>X_train</code> that only contains numeric columns.</p>
<pre class="python"><code>X_train_numeric = X_train.select_dtypes(&quot;number&quot;).copy()
X_train_second_model = X_train_numeric.drop([&quot;prod_id&quot;, &quot;num_reviews&quot;, &quot;star_rating&quot;], axis=1).copy()

second_model = LinearRegression()

second_model_scores = cross_validate(
    estimator=second_model,
    X=X_train_second_model,
    y=y_train,
    return_train_score=True,
    cv=splitter
)

print(&quot;Current Model&quot;)
print(&quot;Train score:     &quot;, second_model_scores[&quot;train_score&quot;].mean())
print(&quot;Validation score:&quot;, second_model_scores[&quot;test_score&quot;].mean())

# Current Model
# Train score:      0.7884552982196166
# Validation score: 0.755820363666055

print(&quot;Baseline Model&quot;)
print(&quot;Train score:     &quot;, baseline_scores[&quot;train_score&quot;].mean())
print(&quot;Validation score:&quot;, baseline_scores[&quot;test_score&quot;].mean())

# Current Model
# Train score:      0.7884552982196166
# Validation score: 0.755820363666055</code></pre>
</div>
<div id="selecting-features-based-on-p-values" class="section level3">
<h3>Selecting Features Based on p-values</h3>
<p>In the previous model, both <code>piece_count</code> and <code>min_age</code> had p-values less then 0.05. This model is built using just those two features.</p>
<pre class="python"><code>significant_features = [&quot;piece_count&quot;, &quot;min_age&quot;]

third_model = LinearRegression()
X_train_third_model = X_train[significant_features]

third_model_scores = cross_validate(
    estimator=third_model,
    X=X_train_third_model,
    y=y_train,
    return_train_score=True,
    cv=splitter
)

print(&quot;Current Model&quot;)
print(&quot;Train score:     &quot;, third_model_scores[&quot;train_score&quot;].mean())
print(&quot;Validation score:&quot;, third_model_scores[&quot;test_score&quot;].mean())
# Train score:      0.7869252233899845
# Validation score: 0.7638761794341223


print(&quot;Second Model&quot;)
print(&quot;Train score:     &quot;, second_model_scores[&quot;train_score&quot;].mean())
print(&quot;Validation score:&quot;, second_model_scores[&quot;test_score&quot;].mean())
# Train score:      0.7884552982196166
# Validation score: 0.755820363666055


print(&quot;Baseline Model&quot;)
print(&quot;Train score:     &quot;, baseline_scores[&quot;train_score&quot;].mean())
print(&quot;Validation score:&quot;, baseline_scores[&quot;test_score&quot;].mean())
# Train score:      0.7785726407224942
# Validation score: 0.7793473618106956</code></pre>
</div>
<div id="selecting-features-with-sklearn.feature_selection" class="section level3">
<h3>Selecting Features with <code>sklearn.feature_selection</code></h3>
<p>This model uses <code>RFECV</code> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV">documentation here</a>). “RFE” stands for “recursive feature elimination”, meaning that it repeatedly scores the model, finds and removes the feature with the lowest “importance”, then scores the model again. If the new score is better than the previous score, it continues removing features until the minimum is reached. “CV” stands for “cross validation” here, and we can use the same splitter we have been using to test our data so far.</p>
<pre class="python"><code>from sklearn.feature_selection import RFECV
from sklearn.preprocessing import StandardScaler

# Importances are based on coefficient magnitude, so
# we need to scale the data to normalize the coefficients
X_train_for_RFECV = StandardScaler().fit_transform(X_train_second_model)

model_for_RFECV = LinearRegression()

# Instantiate and fit the selector
selector = RFECV(model_for_RFECV, cv=splitter)
selector.fit(X_train_for_RFECV, y_train)</code></pre>
</div>
</div>
<div id="build-and-evaluate-a-final-predictive-model" class="section level2">
<h2>Build and Evaluate a Final Predictive Model</h2>
<p>Create a list <code>best_features</code> which contains the names of the best model features based on the findings of the previous step:</p>
<pre class="python"><code>best_features = [&quot;piece_count&quot;, &quot;max_age&quot;, &quot;difficulty_level&quot;]
X_train_final = X_train[best_features]
X_test_final = X_test[best_features]

final_model = LinearRegression()

# Fit the model on X_train_final and y_train
final_model.fit(X_train_final, y_train)

# Score the model on X_test_final and y_test
# (use the built-in .score method)
final_model.score(X_test_final, y_test)
# 0.6542913715071492

from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, final_model.predict(X_test_final), squared=False)
# 47.403687974333</code></pre>
<p>Attribution: <a href="https://github.com/learn-co-curriculum/dsc-linear-regression-lab">The Flatiron School</a></p>
</div>
</div>
